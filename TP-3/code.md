# DÃ©tection d'Intrusions avec RÃ©seaux de Neurones Artificiels (ANN)

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente un systÃ¨me de dÃ©tection d'intrusions utilisant des rÃ©seaux de neurones artificiels (ANN) avec TensorFlow/Keras. Il compare deux architectures contrastÃ©es (shallow vs deep) sur le dataset NSL-KDD.

---

## ğŸ¯ Objectifs

- DÃ©tecter les attaques rÃ©seau (DoS, Probing, R2L, U2R)
- Comparer les performances entre architectures simples et profondes
- Analyser l'impact des hyperparamÃ¨tres sur l'apprentissage

---

## ğŸ“¦ DÃ©pendances

```bash
pip install tensorflow pandas scikit-learn matplotlib numpy
```

---

## ğŸ” Explication du Code Ligne par Ligne

### **Section 0 : Imports et ReproductibilitÃ©**

```python
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
```
- **Imports des bibliothÃ¨ques** : pandas pour la manipulation des donnÃ©es, scikit-learn pour le preprocessing, TensorFlow pour les rÃ©seaux de neurones, matplotlib pour la visualisation.

```python
try:
    keras = tf.keras
    layers = tf.keras.layers
except Exception:
    import keras as _keras
    keras = _keras
    layers = _keras.layers
```
- **Gestion de compatibilitÃ©** : Utilise `tf.keras` en prioritÃ©, sinon bascule vers Keras standalone pour Ã©viter les erreurs d'import.

```python
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
```
- **ReproductibilitÃ©** : Fixe les graines alÃ©atoires pour que les rÃ©sultats soient identiques Ã  chaque exÃ©cution.

---

### **Section 1 : Chargement des DonnÃ©es**

```python
data_url = "https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt"
df = pd.read_csv(data_url, header=None)
```
- **TÃ©lÃ©chargement** : Charge le dataset NSL-KDD (donnÃ©es d'entraÃ®nement) depuis GitHub.
- `header=None` : Le fichier n'a pas de ligne d'en-tÃªte.

```python
cols = ['duration', 'protocol_type', 'service', ...]
df.columns = cols
```
- **Attribution des noms de colonnes** : Les 43 colonnes reprÃ©sentent les caractÃ©ristiques des connexions rÃ©seau (durÃ©e, protocole, service, etc.).

```python
print("Shape:", df.shape)
print(df.head())
```
- **Inspection** : Affiche la forme du dataset (nombre de lignes Ã— colonnes) et les 5 premiÃ¨res lignes.

---

### **Section 2 : Exploration des DonnÃ©es**

```python
print(df.info())
print("\nLabel distribution:\n", df['label'].value_counts())
```
- `df.info()` : Affiche le type de chaque colonne et les valeurs manquantes.
- `value_counts()` : Compte le nombre d'Ã©chantillons "normal" vs "attaques".

---

### **Section 3 : PrÃ©traitement**

```python
df = df.drop(columns=['difficulty'])
```
- **Suppression** : La colonne `difficulty` n'est pas utilisÃ©e pour l'entraÃ®nement.

```python
X = df.drop(columns=['label'])
y = df['label'].apply(lambda s: 0 if s == 'normal' else 1)
```
- **SÃ©paration X/y** : 
  - `X` contient toutes les features (caractÃ©ristiques).
  - `y` contient les labels encodÃ©s en binaire (0 = normal, 1 = attaque).

```python
X = pd.get_dummies(X, columns=['protocol_type', 'service', 'flag'])
```
- **Encodage One-Hot** : Transforme les variables catÃ©gorielles en colonnes binaires.
  - Exemple : `protocol_type = 'tcp'` devient `protocol_type_tcp = 1`.
  - Fait passer le nombre de features de 41 Ã  ~122.

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```
- **Normalisation** : Centre les donnÃ©es (moyenne = 0) et rÃ©duit leur Ã©chelle (Ã©cart-type = 1).
- Essentiel pour les rÃ©seaux de neurones (convergence plus rapide).

```python
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y.values, test_size=0.2, random_state=SEED, stratify=y.values
)
```
- **Division train/test** : 80% entraÃ®nement, 20% test.
- `stratify` : Conserve la proportion de classes dans chaque ensemble.

---

### **Section 4 : Construction du ModÃ¨le**

```python
def build_model(input_dim, n_hidden_layers=1, n_neurons=32, learning_rate=0.001, dropout_rate=0.0):
```
- **Fonction gÃ©nÃ©rique** : CrÃ©e des architectures personnalisables.

```python
model = keras.Sequential()
model.add(layers.Input(shape=(input_dim,)))
```
- **ModÃ¨le sÃ©quentiel** : Les couches sont empilÃ©es linÃ©airement.
- `Input` : SpÃ©cifie la taille d'entrÃ©e (122 features).

```python
for i in range(n_hidden_layers):
    model.add(layers.Dense(n_neurons, activation='relu'))
    if dropout_rate > 0:
        model.add(layers.Dropout(dropout_rate))
```
- **Couches cachÃ©es** : Boucle pour ajouter `n_hidden_layers` couches.
  - `Dense` : Couche fully-connected avec `n_neurons` neurones.
  - `relu` : Fonction d'activation (Rectified Linear Unit).
  - `Dropout` : DÃ©sactive alÃ©atoirement `dropout_rate` % des neurones (rÃ©gularisation).

```python
model.add(layers.Dense(1, activation='sigmoid'))
```
- **Couche de sortie** : 1 neurone avec activation `sigmoid` (sortie entre 0 et 1).
- InterprÃ©tÃ© comme la probabilitÃ© d'attaque.

```python
optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
```
- **Compilation** :
  - `Adam` : Optimiseur adaptatif (ajuste automatiquement le taux d'apprentissage).
  - `binary_crossentropy` : Fonction de perte pour classification binaire.
  - `metrics=['accuracy']` : Suit la prÃ©cision pendant l'entraÃ®nement.

---

### **Section 5 : DÃ©finition des Architectures**

```python
m1_params = {
    'n_hidden_layers': 1,
    'n_neurons': 4,
    'learning_rate': 0.05,
    'dropout_rate': 0.0,
    'batch_size': 512,
    'epochs': 15
}
```
- **ModÃ¨le Shallow (simple)** :
  - 1 seule couche cachÃ©e avec seulement 4 neurones.
  - Taux d'apprentissage Ã©levÃ© (0.05).
  - Batch size large (512).
  - **ProblÃ¨me** : Goulot d'Ã©tranglement (122 features â†’ 4 neurones).

```python
m2_params = {
    'n_hidden_layers': 3,
    'n_neurons': 32,
    'learning_rate': 0.001,
    'dropout_rate': 0.2,
    'batch_size': 64,
    'epochs': 15
}
```
- **ModÃ¨le Deep (profond)** :
  - 3 couches cachÃ©es avec 32 neurones chacune.
  - Taux d'apprentissage faible (0.001).
  - Dropout de 20% (Ã©vite le surapprentissage).
  - Batch size modÃ©rÃ© (64).

---

### **Section 6 : Construction et RÃ©sumÃ©s**

```python
model_shallow = build_model(input_dim, **{k:v for k,v in m1_params.items() if k in [...]})
model_deep = build_model(input_dim, **{k:v for k,v in m2_params.items() if k in [...]})
```
- **Filtrage des paramÃ¨tres** : Extrait uniquement les paramÃ¨tres nÃ©cessaires pour `build_model`.

```python
model_shallow.summary()
model_deep.summary()
```
- **Affichage de l'architecture** : Montre le nombre de paramÃ¨tres entraÃ®nables.

---

### **Section 7 : EntraÃ®nement**

```python
history_shallow = model_shallow.fit(X_train, y_train,
                                   validation_split=0.2,
                                   epochs=m1_params['epochs'],
                                   batch_size=m1_params['batch_size'],
                                   verbose=1)
```
- `fit()` : Lance l'entraÃ®nement.
- `validation_split=0.2` : Utilise 20% des donnÃ©es d'entraÃ®nement pour la validation.
- `epochs` : Nombre de passages complets sur les donnÃ©es.
- `batch_size` : Nombre d'Ã©chantillons traitÃ©s avant la mise Ã  jour des poids.
- `verbose=1` : Affiche la progression.

---

### **Section 8 : Ã‰valuation**

```python
test_loss_sh, test_acc_sh = model_shallow.evaluate(X_test, y_test, verbose=0)
test_loss_de, test_acc_de = model_deep.evaluate(X_test, y_test, verbose=0)
```
- **Test** : Calcule la perte et la prÃ©cision sur les donnÃ©es de test (non vues pendant l'entraÃ®nement).

```python
print(f"Shallow test acc: {test_acc_sh:.4f} | Deep test acc: {test_acc_de:.4f} | Diff: {test_acc_de - test_acc_sh:.4f}")
```
- **Comparaison** : Affiche la diffÃ©rence de performance entre les deux modÃ¨les.

---

### **Section 9 : Visualisation**

```python
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history_shallow.history['accuracy'], label='Shallow train acc')
plt.plot(history_deep.history['accuracy'], label='Deep train acc')
```
- **Courbes d'entraÃ®nement** : Visualise l'Ã©volution de la prÃ©cision au fil des epochs.

```python
plt.subplot(1,2,2)
plt.plot(history_shallow.history['val_accuracy'], label='Shallow val acc')
plt.plot(history_deep.history['val_accuracy'], label='Deep val acc')
```
- **Courbes de validation** : DÃ©tecte le surapprentissage (si train >> val).

```python
plt.figure(figsize=(6,4))
accs = [test_acc_sh, test_acc_de]
names = ['Shallow', 'Deep']
bars = plt.bar(names, accs)
for bar,acc in zip(bars,accs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f"{acc:.4f}", ha='center', va='bottom')
```
- **Graphique Ã  barres** : Compare visuellement les prÃ©cisions de test avec annotations.

---

## ğŸ“ Concepts ClÃ©s

### **Pourquoi le modÃ¨le shallow performe mal ?**
1. **Goulot d'Ã©tranglement** : 122 features â†’ 4 neurones = perte d'information massive.
2. **CapacitÃ© d'apprentissage limitÃ©e** : Trop peu de paramÃ¨tres pour capturer la complexitÃ©.
3. **HyperparamÃ¨tres inadaptÃ©s** : Learning rate trop Ã©levÃ©, batch size trop grand.

### **Pourquoi le modÃ¨le deep performe mieux ?**
1. **Plus de capacitÃ©** : 32 neurones par couche.
2. **Apprentissage hiÃ©rarchique** : 3 couches permettent d'extraire des features Ã  diffÃ©rents niveaux d'abstraction.
3. **RÃ©gularisation** : Dropout Ã©vite le surapprentissage.
4. **Meilleurs hyperparamÃ¨tres** : Learning rate faible + batch size modÃ©rÃ© = convergence stable.

---

## ğŸ“Š RÃ©sultats Attendus

- **Shallow Network** : ~75-85% de prÃ©cision
- **Deep Network** : ~90-95% de prÃ©cision
- **AmÃ©lioration** : +10-15 points de pourcentage

---

## ğŸš€ Utilisation

1. **ExÃ©cuter le code complet** :
```bash
python script.py
```

2. **Analyser les sorties** :
   - Formes des datasets
   - Distribution des labels
   - PrÃ©cisions de test
   - Graphiques de performance

---

## ğŸ“ Notes Importantes

- **ReproductibilitÃ©** : Le SEED=42 garantit des rÃ©sultats identiques.
- **Normalisation** : Essentielle pour la convergence des rÃ©seaux de neurones.
- **Encodage One-Hot** : Transforme les catÃ©gories en features numÃ©riques exploitables.
- **Validation Split** : Permet de surveiller le surapprentissage pendant l'entraÃ®nement.

---

## ğŸ”¬ ExpÃ©rimentations SuggÃ©rÃ©es

1. **DÃ©grader le shallow** : 2 neurones au lieu de 4 â†’ performance encore pire.
2. **AmÃ©liorer le shallow** : 64 neurones + meilleurs hyperparamÃ¨tres â†’ se rapproche du deep.
3. **Retirer le dropout** : Observer l'impact sur le surapprentissage.

---

## ğŸ“š RÃ©fÃ©rences

- **Dataset** : NSL-KDD (Network Security Laboratory - Knowledge Discovery in Databases)
- **Framework** : TensorFlow/Keras
- **PrÃ©processing** : Scikit-learn

---

## ğŸ‘¤ Auteur

Projet acadÃ©mique - TP3 Machine Learning, Deep Learning et SÃ©curitÃ©  
4Ã¨me AnnÃ©e IngÃ©nierie SÃ©curitÃ©, USTHB